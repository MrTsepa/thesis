\documentclass[12pt,a4paper]{extarticle}

\usepackage[top=20mm, bottom=20mm, left=30mm, right=20mm]{geometry}
\usepackage[T1]{fontenc}
\usepackage[utf8x]{inputenc}
\usepackage[russian]{babel}
\usepackage{amsmath}
\usepackage{amssymb}


\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathcal{E}}
\newcommand{\W}{\textbf{W}}
\newcommand{\s}{\textbf{s}}
\newcommand{\Loss}{\mathcal{L}}
\newcommand{\encoder}{\operatorname{enc}}
\newcommand{\decoder}{\operatorname{dec}}
\newcommand{\unit}[1]{\textbf{e}_{#1}}

\title{Анализ проблемы вложения графов и применимости к ней нового метода -- структурной функции потерь}

\begin{document}

    \begin{titlepage}
        \pagenumbering{gobble}
        \clearpage
        \pagestyle{empty}

        \begin{center}
            \textrm{Министерство образования и науки Российской Федерации}
            \\[5mm]
            Федеральное государственное автономное образовательное учреждение\\
            высшего профессионального образования\\
            <<Московский физико-технический институт\\
            {(государственный университет)}>>\\[5mm]
            Факультет радиотехники и кибернетики\\[5mm]
            Кафедра проблем передачи информации и анализа данных\\[50mm]
            \textbf{АНАЛИЗ ПРОБЛЕМЫ ВЛОЖЕНИЯ ГРАФОВ И ПРИМЕНИМОСТИ К НЕЙ НОВОГО МЕТОДА -- СТРУКТУРНОЙ ФУНКЦИИ ПОТЕРЬ}
            \\[8mm]
            \textrm{Выпускная квалификационная работа}
            \\
            (бакалаврская работа)\\[7mm]

            Направление подготовки: 03.03.01 Прикладные математика и физика\\[40mm]
        \end{center}

        \noindent Выполнил:\\
        студент 411а группы \hspace{0.9cm} \underline{\hspace{4cm}} Цепа Станислав Евгеньевич\\[2mm]

        \noindent Научный руководитель:\\
        к.ф.-м.н. \hspace{3.1cm} \underline{\hspace{4cm}} Панов Максим Евгеньевич
        \\[20mm]

        \begin{center}
            Москва 2017
        \end{center}

    \end{titlepage}

    \pagenumbering{arabic}
    \setcounter{page}{2}

    \tableofcontents

    \newpage

    \section{Введение}
    Большинство алгоритмов машинного обучения основано на использовании
    признаков, являющихся числами (а также иногда категориями).
    Однако, далеко не все сущности, которые хотелось бы использовать в качестве признаков,
    можно легко перевести в числовой вид.
    Примерами таких объектов являются слова в тексте или вершины в графе.
    В нашей работе мы рассматривали задачу вложения: как каждой отдельной сущности (в нашем
    случае вершине графа) сопоставить точку вещественного $n$-мерного пространства,
    или, проще говоря, $n$ вещественных чисел, чтобы относительное
    расположение этих точек лучшим образом отображало структуру графа.
    Мы проверяли качество вложения, проводя TODO классификацию и кластеризацию
    стандартными методами и вычисляя соответствующие метрики.
    Стоить отметить, что вложение является задачей обучения без учителя,
    то есть при построении не учитывает известные метки классов, но только
    переводит вершины в числовые признаки.
    После этого полученные числовые признаки могут быть использованы для любых целей, в том числе и для
    применения в алгоритмах классификации и кластеризации.

    \section{Постановка задачи}
    В данном разделе мы формализуем задачу вложения графов и демонстрируем как можно разделить ее на подзадачи.

    Пусть дан граф $G = (V, E)$, $n = |V|$ его матрица смежности $A \in \R^{n \times n},\ A_{ij} \in \{0, 1\}$.
    
    Также вводится матрица схожести $S,\ S_{ij} \in \R_{+}$,  каждый элемент которой оценивает некую "схожесть" двух вершин. Обычно матрицу схожести получают из матрицы смежности таким образом чтобы учитывалось соседство второго и более высших порядков. Стоит заметить, что не во всех алгоритмах возможно использовать небинарную матрицу, например в нашем алгоритме это возоможно только при некоторых усложнениях, поэтому, хоть далее мы и будем для общности везде указывать матрицу $S$, в основном будет подразумеваться $S = A$. TODO
    
    Алгоритм, строящий вложение графа по этим данным, обычно состоит из следующих частей:
    \begin{itemize}
        \item функция кодирования (\textit{encoder}) строит вложение графа
            \[\encoder: V \to \R^{n \times d}.\]
            далее будем обозначать вложение $i$ вершины $\E_i = \encoder(v_i)$
        \item функция декодирования (\textit{decoder}) оценивает схожесть двух элементов вложения,
            в идеальном случае $\decoder(\E_i, \E_j) = S_{ij}$
            \[\decoder: \R^{d} \times \R^{d} \to \R_+\]
        \item функция потерь (\textit{loss}) $\Loss$ оценивает насколько хорошо функции декодирования удалось восстановить исходную матрицу схожести.
    \end{itemize}
    
    Таким образом задача вложения графа сводится к поиску подходящего функционала кодирования и выбора правильной функции потерь, после чего решается задача оптимизации, например при помощи градиентного спуска или аналитически. Далее мы рассмотрим каждую из составляющих алгоритма подробнее.

    \subsection{Функция кодирования}
    Запишем еще раз определение функции кодирования
    \begin{equation}
        \encoder(v_i) = \E\unit{i}.
    \end{equation}
    Здесь $\E$ -- матрица вложения размера $n \times d$, а $\unit{i}$ -- единичный вектор. Можно представить
    \begin{equation} \label{enc_eq_f}
        \encoder(v_i) = f(\s_i),
    \end{equation}
    где $f$ -- произовольная функция $\R^n \to \R^d$, а $\s_i$ -- $i$ столбец матрицы схожести. Этот подход реализуется, например, при использовании нейросетей TODO.
    Если принять, что $f$ линейная функция, то \eqref{enc_eq_f} превратится в
    \begin{equation}
        \encoder(v_i) = \W\s_i + \textbf{b}.
    \end{equation}
    или
    \begin{equation}
        \E = \W S + \textbf{b}.
    \end{equation}
    
    Здесь размер матриц $\W$ и $\textbf{b}$ это $n \times d$. Этой формулой мы и будем пользоваться в нашем алгоритме.
    Стоит заметить что если матрица $S$ имеет ранг хотя бы $d$ то матрица $\E$ может принимать любые значения.

    \subsection{Функция декодирования}
    Функция декодирования обычно имеет простой вид, поскольку она просто представляет из себя вычисление какого либо расстояния между двумя $d$-мерными векторами. Вот примеры такой функции:
    \begin{itemize}
        \item скалярное произведение
            \[\decoder(\E_i, \E_j) = \E_i^T\E_j\]
        \item корреляция, нормированное скалярное произведение
            \[\decoder(\E_i, \E_j) =  \frac{\E_i^T\E_j}{|\E_i||\E_j|} \]
        \item софтмакс??
            \[\decoder(\E_i, \E_j) = \frac{e^{\E_i^T\E_j}}{\sum_{k \in V} e^{\E_i^T\E_k}}\]
    \end{itemize}
    
    \subsection{Функция потерь}
    Существующие функции потерь могут быть условно разделены на два класса: попарная функция потерь и глобальная функция потерь
    
    \subsubsection{Попарная функция потерь}
    Эта функция потерь представляет из себя сумму ошибок по каждому элементу $S$
    
    \[\Loss = \sum_{i, j \in V} \ell (\decoder(\encoder(\E_i), \encoder(\E_j)), S_{ij}) \]
    
    обычно функция $\ell$ представляет из себя квадратичную ошибку 
    \[\ell(x, y) = \lVert x - y \rVert_2^2 \]
    
    \subsubsection{Глобальная функция потерь}
    В некоторых случаях функция потерь имеет более сложный вид и не может быть разложена в сумму, каждый такой случай можно рассматривать отдельно. К такому виду функции потерь относится и структурная функция потерь -- она пытается вычислять оптимальность вложения основываясь на его статистических характеристиках.

    \bibliography{main}
    \bibliographystyle{plain}

\end{document}
